{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Greetings traveller! \n",
    "Welcome to Week 1 of the AIAP coursework. In the next 6 weeks, you will be *travelling* through the machine learning *space*, where we will be looking at various different machine learning problems and how to solve them. We will be taking a quick sweep through the many areas of analytics and machine learning, including learning about learning from data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Course Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This course is designed to be different from the courses you have done previously. We acknowledge that there are lots of great free resources online, and are not trying to create yet another tutorial. Rather, we attempt to provide something that complements them, mirroring real-life problem solving. In general, we have the following goals out of the programme:\n",
    "- __Finger-dipping exposure into ML__: we concede that six weeks are insufficient to fully understand ML, and we do not aim to do that. Rather, we would like to provide sufficient breadth in terms of practical, useful knowledge in the area.\n",
    "- __Confidence to go further yourself__: the ML space is vast and expanding every day, no practitioner is ever sufficently trained to tackle any problem. Rather, good data scientists hone a sharp ability to learn new techniques to solve novel problems. We wish to build your confidence to go into the unknown, so that you can rely on yourself for learning beyond textbook knowledge.\n",
    "- __Programming competency__: while we are not training software engineers, writing good, clean code is crucial to the success of any project that requires programming. We will provide guidance on how to write reproducible, human-centric code for data science that will pay dividends for a project in the long term.\n",
    "- __Employability__: ultimately, we aim to help you get hired in the data science space, and we have crafted our notebooks to act like mock technical assessments in a safe space. The six week programme will equip you with critical soft skills for data science as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Prerequisite Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is certainly not a course for beginners. Considering that this will be a full-time, six-week programme, we have designed the course to be challenging in every aspect. Realistically, we would not expect anyone to be comfortable handling the course. Although the programme will be challenging, we wish to cultivate a forgiving, learning-based culture where necessary failure is celebrate encouraged and celebrated. Nevertheless, the following prerequisites will help you do well in this course:\n",
    "- __Python, or general programming skills__: an ability to execute basic tasks beyond hello world in Python, or simply being comfortable with computer languages in general\n",
    "- __Numerical programming__: an ability to execute mathematical scripts through a programming language like Python (it will be relatively easy to transition from Matlab, R, SAS or Julia)\n",
    "- __Statistical fundamentals__: you should know how basic tools like linear and logistic regression work, and have a mathematical appreciation for it\n",
    "- __Linear algebra and calculus__: basic mathematical knowledge will help you appreciate the algorithms and learn to use them better\n",
    "- __A positive learning attitude__ most importantly, because realistically, no one will have all of the above, so we will all need to adapt and learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Learning Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At one or more points in time while attempting this notebook, you may find the following resources to be useful:\n",
    "- Intro to Python Programming: [Python at PluralSight](https://app.pluralsight.com/paths/skills/python)*\n",
    "- Numerical computing in Python: [Python for Data Science, 2nd Ed. by Wes McKinney](http://wesmckinney.com/pages/book.html)\n",
    "- Random Forests in depth and from scratch: [fast.ai Machine Learning Course](https://course.fast.ai/ml)\n",
    "- Organising your notebook: [initial steps toward reproducible research by Karl Broman](https://kbroman.org/steps2rr/)\n",
    "- Finding specific methods in Pandas: [Pandas documentation](http://pandas.pydata.org/pandas-docs/stable/)\n",
    "- All the answer keys you'll ever need: [Kaggle's Titanic Kernel](https://www.kaggle.com/c/titanic/kernels)\n",
    "\n",
    "*You'll get free access to Pluralsight's Python track through your DataCamp subscription!\n",
    "\n",
    "These are just recommended resources - do tap on anything you find useful, or approach us for alternative recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Collaboration Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Collaboration is the best way to learn. In short, we should optimise learning - the general rule is to try everything yourself first, then discuss your method with your teammates. Do not directly copy their code, unless you are trying to learn a programming technique. Be smart and flexible - do your own thinking and write your own code.\n",
    "\n",
    "If you believe referring to someone's answer is the best way to learn, we recommend looking at the code, then walking away for a few minutes, and come back to write your own version of the code. There will be minimum policing, but we expect no plagurism or direct copying of code to occur.\n",
    "\n",
    "Please list your collaborators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "1. John Smith\n",
    "2. ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Time Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this notebook, there will two main areas of focus: modelling, and model implementation (numerical programming). Different people will have different strengths, and our advice is to play to your strengths, and collaborate to learn from people who can teach you something, but offer something in return. Unless you are really good with everything the notebook needs you to do, __you are not likely to finish the notebook by doing it alone__.\n",
    "\n",
    "For those who are very new to programming, it's okay to realise that you might not finish the notebook this week. If you are feel that you might not be ready to mix different technical fields together, considering spending more time building up your fundamentals instead - take your time if you need to. If you need more assistance, don't hesitate to speak to us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, let's get into week one of the course! __Good luck, have fun.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Initial Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our problem this week is the el classico: the Titanic dataset, a dataset probably done to death by fellow travellers of the machine learning space. There is a reason why this dataset is so popular - it demands for all the fundamentals required of statistical modelling, while staying light in terms of technical demands. With just 891 rows of data, the problem can be solved on any laptop. While your laptop would not face much stress this week, we would recommend you to consider your technical set-up, so that as heavier datasets come about (in the size of GBs), you will not be limited by them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, go grab the data. The data is available at https://www.kaggle.com/c/titanic/data, you will need to register an account to retrieve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since we are talking about downloading data, we should take this time to set up your folder. One such way (our recommended way) to do it as to create a project folder, then leave your notebooks in the root of that folder. Your code base, which we will be starting to build over time, should be in a `src` subfolder, while your data should be in a `data` folder, with a tree structure as shown:\n",
    "\n",
    "```\n",
    "aiap\n",
    " |- src\n",
    " |- data\n",
    " |   |- titanic.csv\n",
    " |   |- titanic_test.csv\n",
    " |- week1.ipynb\n",
    "```\n",
    "\n",
    "This is an opinionated format - we suggest this only for simplicity reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, we are ready to do some coding work. First, import the necessary libraries you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Import your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Initial Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Train a stock random forest model (with no custom parameters and report the accuracy score). Do the __minimal__ cleaning required to let your model fit into the model. Using the `train_test_split` method, reserve some validation data for evaluation use.\n",
    "\n",
    "You may see an accuracy of approximately 75% - this does not mean anything substantially, but it lets us know that a no-value-add approach to modelling will already generate this accuracy for us. Hence, our goal is to improve upon this current score, and reach as high as possible. For now, 75% is good enough for us to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of initial model: 82.1%\n"
     ]
    }
   ],
   "source": [
    "# Clean Data by filling NaNs with dummy values\n",
    "dataset_nonans = dataset.copy()\n",
    "dataset_nonans['Age'].fillna(999, inplace=True)\n",
    "dataset_nonans['Cabin'].fillna('NA', inplace=True)\n",
    "dataset_nonans['Embarked'].fillna('NA', inplace=True)\n",
    "X = dataset_nonans.drop('Survived', axis=1)\n",
    "y = dataset_nonans['Survived']\n",
    "\n",
    "# Encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder_name = LabelEncoder()\n",
    "X['Name'] = label_encoder_name.fit_transform(X['Name'])\n",
    "labelencoder_sex = LabelEncoder()\n",
    "X['Sex'] = labelencoder_sex.fit_transform(X['Sex'])\n",
    "labelencoder_ticket = LabelEncoder()\n",
    "X['Ticket'] = labelencoder_ticket.fit_transform(X['Ticket'])\n",
    "labelencoder_cabin = LabelEncoder()\n",
    "X['Cabin'] = labelencoder_cabin.fit_transform(X['Cabin'])\n",
    "labelencoder_embark = LabelEncoder()\n",
    "X['Embarked'] = labelencoder_embark.fit_transform(X['Embarked'])\n",
    "X_dummies = pd.get_dummies(X, columns=['Embarked'], drop_first=True)\n",
    "\n",
    "# Split data into Training and Test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=2018,\n",
    "                                                    stratify=y)\n",
    "\n",
    "# Fit random forest without paramter tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_initial = RandomForestClassifier()\n",
    "rf_initial.fit(X_train, y_train)\n",
    "\n",
    "# Determine accuracy using test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_initial = accuracy_score(y_test, rf_initial.predict(X_test))\n",
    "print('Accuracy of initial model: {:.1f}%'.format(accuracy_initial * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Exploring the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Overall Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Conduct some initial exploration of this data. This could be through dataset level plots, correlation charts and table describes, as well as by understanding what kind of information is available, or not available (i.e. missing). Write a paragraph on what you observe in the data. There is no correct answer, but do present useful and insightful information as much as possible.\n",
    "\n",
    "This part of the notebook should be helpful to someone who is trying to come into your project, but has no knowledge of the data. In complex, real-world problems, there may be multiple data sources, each with different structures of data. Making sense of data at this macro level may happen over several months in an iterative manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      "Pclass      891 non-null int64\n",
      "Name        891 non-null object\n",
      "Sex         891 non-null object\n",
      "Age         714 non-null float64\n",
      "SibSp       891 non-null int64\n",
      "Parch       891 non-null int64\n",
      "Fare        891 non-null float64\n",
      "Cabin       204 non-null object\n",
      "Embarked    889 non-null object\n",
      "dtypes: float64(2), int64(3), object(4)\n",
      "memory usage: 62.7+ KB\n",
      "          Pclass       Age     SibSp     Parch      Fare\n",
      "Pclass  1.000000 -0.369226  0.083081  0.018443 -0.549500\n",
      "Age    -0.369226  1.000000 -0.308247 -0.189119  0.096067\n",
      "SibSp   0.083081 -0.308247  1.000000  0.414838  0.159651\n",
      "Parch   0.018443 -0.189119  0.414838  1.000000  0.216225\n",
      "Fare   -0.549500  0.096067  0.159651  0.216225  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Split into features and target\n",
    "X = dataset.drop(['PassengerId', 'Ticket', 'Survived'], axis=1)\n",
    "y = dataset['Survived']\n",
    "\n",
    "# Checking for missing data\n",
    "X.info()\n",
    "\n",
    "# Correlation matrix\n",
    "print(X.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. There is missing data in the 'Age', 'Cabin' and 'Embarked' Columns. Specifically there are 177 NaNs in 'Age', 687 NaNs in 'Cabin' and 2 NaNs in 'Embarked'.\n",
    "2. The variables 'Age', 'SibSp', 'Parch' and 'Fare' are numerical, while the variables 'Pclass', 'Sex', 'Cabin' and 'Embarked' are categorical.\n",
    "3. This is moderate correlation between (Pclass, Age), (Pclass, Fare), (SibSp, Age), (SibSp, Parch) pairs. The values of correlation for these variable pairs are between 0.3 and 0.6. These values can be either positive or negative.\n",
    "\n",
    "'PassengerId' and 'Ticket' were removed as it does not make sense to include such information in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Individual Variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now go deeper into individual variables. For each variable, provide plots, tables or descriptions that best capture the nuance of that column. There is no correct answer, but there is a gold standard.\n",
    "\n",
    "Samples of this can be found in Kaggle's kernels page. While we value pretty charts, we value insights much more. Where insightful information is found, please indicate them in your notebook for your reader.\n",
    "\n",
    "You may wish to strategically go deeper into variables you find more interesting. There is no need to scrutinize every variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### PassengerId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As mentioned before, it does not make sense to include 'PassengerId', so it will not be included in the subsequent processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the target variable for this problem, i.e. the variable to be predicted. It is categorical, and takes the values of 0 and 1.\n",
    "\n",
    "0 - non-survival, 1 - survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentages:\n",
      "0    61.616162\n",
      "1    38.383838\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show percentages of Survived\n",
    "print('Percentages:')\n",
    "print(y.value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the counts of observations in each class, it appears that there is some imbalance of classes in the target variable, i.e. the non-survival class having more observations than the other class. Thus a model trained on this dataset might be somewhat biased to the non-survival class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is one of the predictors for this problems. It is categorical, and takes on the values of 1, 2 and 3.\n",
    "\n",
    "1 - 1st class, 2 - 2nd class, 3 - 3rd class.\n",
    "\n",
    "There is no need for one-hot encoding of this variable, as it is ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentages:\n",
      "3    55.106622\n",
      "1    24.242424\n",
      "2    20.650954\n",
      "Name: Pclass, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show percentages of Pclass\n",
    "print('Percentages:')\n",
    "print(X['Pclass'].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that slightly more than half of the passengers hold 3rd class tickets, while there are slightly more 1st class ticket holders than 2nd class ticket holders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The name would not be used directly in the model building process, but the salutation (Mr., Mrs. etc) contained in each value could be used to help fill in missing values in the 'Age' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is one of the predictors for this problems. It is categorical, and takes on the values of 0 and 1.\n",
    "\n",
    "0 - female, 1 - male.\n",
    "\n",
    "There is no need for one-hot encoding of this variable as it is in essence a dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentages:\n",
      "male      64.758698\n",
      "female    35.241302\n",
      "Name: Sex, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show percentages of Sex\n",
    "print('Percentages:')\n",
    "print(X['Sex'].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there are somewhat more male passengers compared to female passengers (65% vs 35%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is one of the predictors for this problems. It is numerical, thus no encoding is needed for this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram for Age\n",
    "X['Age'].dropna(inplace=False).hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Age')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEcCAYAAAA7neg3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHThJREFUeJzt3X+8VXWd7/HXW0BRcUQ0z6B2xUoNtXTqjGU5dZDqhv3QW/mDrIvFSAxFP2weaTpzRxsxmMdcuz1o1ChMskTQdGQgSaOzx+wHBqaWnkozTIJERcxDoR783D/W9+D2uA98z+Gss89mv5+Px36c9Xt99mKx3vu71tp7KSIwMzPLsVu9CzAzs8bh0DAzs2wODTMzy+bQMDOzbA4NMzPL5tAwM7NsDg0bdJKulnRJveuot+1tB0lnS7pjsGvaWZIukPT1EpbbkNtjV+TQaGKS1kj6i6ROSU9KWibp5fWuq5qkkPSqetfRyCQdLenW9G+8SdJqSSeXsa6IuDQi/r6MZdvQ4NCw90TEKGAs8Cgwt871lEaFZtzn/wu4DWgBDgQ+CfypPwuSNGwA67IG1Iz/gayGiNgC3AAc1T1M0r6SvinpMUkPS/qn7oOupCsk3VA17RxJK9KBuU3S2nSq4vHUojmrt3VLOkfSg5I2Sloi6aA0/PY0yT2pNXRGjXmHSfq/aT2/k/SJ1DoZnsZXJM2S9CPgz8ArJB2U1rMxrfecquW96JRR93up6l8j6fOS7k+f3L8haWTV+HdLujt9ov+xpNdWjfsbSXdJelrSImDbfL1vGs2V9JSkX0mamAaeJml1jwk/K+k/ayzgAOAw4GsR8Wx6/Sgi7kjjX3Lap7p1l7bHFZK+K2kz8HlJf6wOD0n/S9K9qfsiSd9K3cslfaLHsu+R9L7U/WpJt6V/h19LOr1quv3Tv9GfJN0JvHIH28oGiUPDAJC0F3AG8NOqwXOBfYFXAG8F/jfwkTTus8Br00Hn74CpwJR44Xdp/ho4ADgYmALMk3RkjfWeBHwROJ2itfMwcB1ARLwlTXZsRIyKiEU1Sj8HmAQcB7wOOLXGNB8GpgH7pOUvBNYCBwEfAC7tPiBnOgv4nxQHsiOAf0rv5XXAVcDHgP2BrwJLJO0haXfgP4FrgDHA9cD7d7CeNwAPUWzHfwFulDQGWAIcJml81bQfSsvu6QngQeBbkk6V1NKH99ntg8Asiu3378Bm4KQe46+tMd+1wOTuHklHAYcCyyTtTdH6uZai9TMZuFzS0Wny/wC2UOwTH00vGwoiwq8mfQFrgE5gE9AFrANek8YNA54Bjqqa/mNApar/eGAjxYF4ctXwtrS8vauGLQb+OXVfDVySuucD/1Y13SjgOWBc6g/gVdt5Dz8APlbV/7Y0z/DUXwG+UDX+5cBWYJ+qYV8Eru5ZW9V7Wdtjm02v6j8Z+G3qvgL41x71/ZoicN+Stq+qxv24el095ju7xvR3Ah+uWtes1H008CSwRy/LOgT4CvBb4HngduDwqvXc0WP6bds8bY9v9hh/CXBV6t6HIkQOTf0XAd/qZdysqvnOAH7YY7lfpQjHYWkfeHXVuEt71ulXfV5uadipETEa2AP4BPDfkrpbCbtTBEK3hylaDgBExJ0Un4RFEQrVnoyIzT3mPajG+g+qXkdEdFJ8Oj64xrS1HAQ8UtX/SI1pqocdBGyMiKd71Ja7vp7Lq35fhwKfTaemNknaRBFSB6XXHyIdAavm3Z5a03evawHwQUmiaEktjohnai0kItZGxCci4pWpxs3AN3f4Ll/Qc5teC7xP0h7A+4C7IuIl7yVt42XAmWnQmcC3U/ehwBt6bKuzKFqoLwOG89LtbEOAQ8MAiIitEXEjxafwE4HHKT7tHVo12f8A/tDdI+njFGGzDvhcj0Xul05BVM+7rsaq11WvI82zf/V6dmA9xSfpbrXu/qo+8K4Dxkjap0dt3evbDOxVNe6vayyveh3V7+sRik//o6tee0XEwlTnwekgXz3v9tSafh1ARPwUeBb4O4rTQ7VOTb1ERDxCcernmDToRe83fWB4yWw9lnE/xUF8Er2fmuq2EJgs6QRgT6A9DX8E+O8e22pURPwD8BhFS7XndrYhwKFhwLY7i04B9gM6ImIrRethlqR9JB0KnAt0X+Q8guI0xYcoPul+TtJxPRZ7saTd0zWPd1Ocx+/pWuAjko5Ln1wvBVZGxJo0/lGKayq9WQx8StLBkkYD523vfaaD5o+BL0oamS5UT+WFT8B3AydLGpMOoJ+usZiPSzokXV+4AOi+1vI1YLqkN6Ttubekd6WA+gnFgfCTkoani8HHb69W0p1OkkZIOg0YD3y3avw3KU47dUW6sN2TpP0kXSzpVZJ2SxfGP8oL167uAY5O238kxemlHNdS3IX1Fmr/u3b7LsWHgi8AiyLi+TR8KXCEpA+n9zdC0t9KGp/2vRuBiyTtla6FTMmsy0rm0LD/ktRJcQvmLIqL2felcTMpPok+BNxBcaC4SsWdSd8C5kTEPRHxAMXB85p04Af4I8V59nUUB+TpEfGrniuPiBXAPwPfofg0/kpeOJ0BxUFsQTqFcXrP+SkO1LcC9wI/pzhIdVG0mHozGRiXarsJ+JeIuC2Nu4biQLomLbfWxfdr07iH0uuS9F5WUVyY/0p67w9SXDMgIp6lOJVzdhp3BsWBcXtWAodTtPpmAR+IiCeqxl9D0WLYXivj2fRev0/xb/xLimtV3XX9huKA/n3gAYp/5xwLKa73/CAiHu9tonTK7EaKa03XVg1/GngHxb/1Oor9ZQ5FyxWKU6Wj0vCrgW9k1mUl04tPmZrtPEltFBdDD9nRtCWsexJwZUQcusOJ+7f8NcDfR8T3y1h+H2vZE9gAvC4Ft1np3NKwhiZpT0knp1M+B1PcfXNTvesaJP8A/MyBYYNpeL0LMNtJAi6mOI30F4q7df5PXSsaBKnFI2p/L8WsND49ZWZm2Xx6yszMsjk0zMwsm0PDzMyyOTTMzCybQ8PMzLI5NMzMLJtDw8zMsjk0zMwsm0PDzMyyNcTPiBxwwAExbty4epexy9m8eTN77733jic0GyK8z5Zj9erVj0fEy3KmbYjQGDduHKtWrap3GbucSqVCW1tbvcswy+Z9thySsp+M6NNTZmaWzaFhZmbZHBpmZpbNoWFmZtlKDQ1Jn5F0n6RfSlooaaSkwyStlPSApEWSdi+zBjMzGzilhUZ69OYngdaIOAYYRvEQ+TnAlyLicOBJYGpZNZjZrmHhwoUcc8wxTJw4kWOOOYaFCxfWu6SmVfYtt8OBPSU9B+wFrAdOAj6Yxi8ALgKuKLkOM2tQCxcu5MILL2T+/Pls3bqVYcOGMXVq8Vlz8uTJda6u+ZTW0oiIPwD/DvyeIiyeAlYDmyKiK022Fji4rBrMrPHNmjWL+fPnM2HCBIYPH86ECROYP38+s2bNqndpTam0loak/YBTgMOATcD1wKQak9Z8SLmkacA0gJaWFiqVSjmFNrHOzk5vVxvyOjo62Lp1K5VKZds+u3XrVjo6Orz/1kGZp6feBvwuIh4DkHQj8CZgtKThqbVxCLCu1swRMQ+YB9Da2hr+FujA87drrRGMHz+eYcOG0dbWtm2fbW9vZ/z48d5/66DMu6d+D7xR0l6SBEwE7gfagQ+kaaYAN5dYg5k1uAsvvJCpU6fS3t5OV1cX7e3tTJ06lQsvvLDepTWl0loaEbFS0g3AXUAX8HOKlsMy4DpJl6Rh88uqwcwaX/fF7pkzZ9LR0cH48eOZNWuWL4LXiSJqXlIYUlpbW8M/WDjwfHrKGo332XJIWh0RrTnT+hvhZmaWzaFhZmbZHBpmZpbNoWFmZtkcGmZmls2hYWZm2RwaTci/GGpm/VX2r9zaEONfDDWzneGWRpPxL4aa2c5waDSZjo4OTjzxxBcNO/HEE+no6KhTRWbWSBwaTWb8+PHccccdLxp2xx13MH78+DpVZGaNxKHRZPyLoWa2M3whvMn4F0PNbGc4NJrQ5MmTmTx5sn8x1Mz6zKenmtDMmTMZOXIkEyZMYOTIkcycObPeJZlZg3BoNJmZM2dy+eWXM3r0aCQxevRoLr/8cgeHmWUpLTQkHSnp7qrXnyR9WtIYSbdJeiD93a+sGuylrrzySkaMGMHGjRuJCDZu3MiIESO48sor612amTWA0kIjIn4dEcdFxHHA64E/AzcB5wMrIuJwYEXqt0HS1dVFV1cXs2fP5pZbbmH27NnbhpmZ7chgnZ6aCPw2Ih4GTgEWpOELgFMHqQZLJk2axLnnnsvIkSM599xzmTRpUr1LMrMGMVh3T50JdP8qXktErAeIiPWSDqw1g6RpwDSAlpYWKpXKYNTZFJYuXcqMGTM46aSTmDFjBkuXLgXwNrYhr7Oz0/tpnSkiyl2BtDuwDjg6Ih6VtCkiRleNfzIitntdo7W1NVatWlVqnc1ixIgR7LbbbkQEzz33HCNGjEASzz//PM8991y9yzPbLt8mXg5JqyOiNWfawTg9NQm4KyIeTf2PShoLkP5uGIQaLJk+fTpdXV2MGTMGgDFjxtDV1cX06dPrXJmZNYLBCI3JvHBqCmAJMCV1TwFuHoQaLJk7dy4zZsxg06ZNAGzatIkZM2Ywd+7cOldmZo2g1NCQtBfwduDGqsGzgbdLeiCNm11mDfZSc+fOZcuWLbS3t7NlyxYHhpllK/VCeET8Gdi/x7AnKO6mMjOzBuNvhJuZWTaHhpmZZXNomJlZNoeGmZll8/M0moCkfs1X9hc/zazxuKXRBCKi5uvQ85b2Os6BYWa1ODTMzCybQ8PMzLI5NMzMLJtDw8yGPD/Xfujw3VNmNqTNnDmTK6+8kjlz5nDUUUdx//33c9555wH4d9PqoPTnaQwEP0+jHOPOX8aa2e+qdxlm2zVy5EhaW1tZtWoVzzzzDHvssce2/i1bttS7vF1CX56n4ZaGmQ1pzzzzDCtXrnxJS8PPta8PX9MwsyHv5JNPftFz7U8++eR6l9S0HBpmNuQtXbqUyy67jC1btnDZZZdte669Db5ST09JGg18HTgGCOCjwK+BRcA4YA1wekQ8WWYdZta4uq9hXHDBBduuaZxwwgn4Omd9lN3S+DKwPCJeDRwLdADnAysi4nBgReo3M6vpnHPOYeXKlVx66aXccsstXHrppaxcuZJzzjmn3qU1pdJaGpL+CngLcDZARDwLPCvpFKAtTbYAqADnlVWHmTW27ttqq1sa06dP9+22dVJmS+MVwGPANyT9XNLXJe0NtETEeoD098ASazCzXYCfaz90lHlNYzjwOmBmRKyU9GX6cCpK0jRgGkBLSwuVSqWUIpudt6sNRRMmTOjXfO3t7QNcifVUZmisBdZGxMrUfwNFaDwqaWxErJc0FthQa+aImAfMg+LLfW1tbSWW2qSWL8Pb1Yai3r507C+k1l9pp6ci4o/AI5KOTIMmAvcDS4ApadgU4OayajAzs4FV9jfCZwLflrQ78BDwEYqgWixpKvB74LSSazAzswFSamhExN1Ard8zmVjmes3MrBz+RriZmWVzaJiZWTaHhpmZZXNomJlZNoeGmZllc2iYmVk2h4aZmWVzaJiZWTaHhpmZZXNomJlZNoeGmZllc2iYmVk2h4aZmWVzaJiZWTaHhpmZZXNomJlZtlIfwiRpDfA0sBXoiohWSWOARcA4YA1wekQ8WWYdZmY2MAajpTEhIo6LiO4n+J0PrIiIw4EVqd/MzBpAPU5PnQIsSN0LgFPrUIOZmfVD2aERwK2SVkualoa1RMR6gPT3wJJrMDOzAVLqNQ3gzRGxTtKBwG2SfpU7YwqZaQAtLS1UKpWSSmxu3q7WaLzP1lepoRER69LfDZJuAo4HHpU0NiLWSxoLbOhl3nnAPIDW1tZoa2srs9TmtHwZ3q7WULzP1l1pp6ck7S1pn+5u4B3AL4ElwJQ02RTg5rJqMDOzgVVmS6MFuElS93qujYjlkn4GLJY0Ffg9cFqJNZiZ2QAqLTQi4iHg2BrDnwAmlrVeMzMrj78RbmZm2RwaZmaWzaFhZmbZHBpmZpbNoWFmZtkcGmZmls2hYWZm2RwaZmaWzaFhZmbZHBpmZpbNoWFmZtkcGmZmlm2HoSGpRdJ8Sbek/qPSL9SamVmTyWlpXA18Dzgo9f8G+HRZBZmZ2dCVExoHRMRi4HmAiOgCtpZalZmZDUk5obFZ0v5AAEh6I/BUqVWZmdmQlPMQpnMpHtH6Skk/Al4GfCB3BZKGAauAP0TEuyUdBlwHjAHuAj4cEc/2uXIzMxt0O2xpRMRdwFuBNwEfA46OiHv7sI5PAR1V/XOAL0XE4cCTgC+qm5k1iJy7p94HvBc4EjgCeI+kiZIOzJj3EOBdwNdTv4CTgBvSJAuAU/tXupmZDbac01NTgROA9tTfBvwUOELSFyLimu3M+/+AzwH7pP79gU3pYjrAWuDgWjNKmgZMA2hpaaFSqWSUan3l7WqNxvtsfeWExvPA+Ih4FIrvbQBXAG8AbgdqhoakdwMbImK1pLbuwTUmjVrzR8Q8YB5Aa2trtLW11ZrMdsbyZXi7WkPxPlt3OaExrjswkg3AERGxUdJz25nvzcB7JZ0MjAT+iqLlMVrS8NTaOARY18/azcxskOWExg8lLQWuT/3vB26XtDewqbeZIuLzwOcBUkvjHyPiLEnXU9x9dR0wBbi5/+Vbt2MvvpWn/rK9DK9t3PnL+jT9vnuO4J5/eUef12Nmu4ac0Pg48D7gxNR/JzA2IjYDE/qxzvOA6yRdAvwcmN+PZVgPT/3lOdbMflef5qlUKn1u6vc1ZMxs17LD0IiIkPRbimsYpwO/A77Tl5VERAWopO6HgOP7WqiZmdVfr6Eh6QjgTGAy8ASwCFBE9Kd1YWZmu4DttTR+BfwQeE9EPAgg6TODUpWZmQ1J2/ty3/uBPwLtkr4maSK1b5k1M7Mm0WtoRMRNEXEG8GqK6xGfAVokXSHJt8+YmTWhnAvhm4FvA9+WNAY4DTgfuLXk2sxsF+bbxBtTzi2320TERuCr6WVm1m++Tbwx+RnhZmaWzaFhZmbZHBpmZpbNoWFmZtkcGmZmls2hYWZm2RwaZmaWzaFhZmbZHBpmZpattNCQNFLSnZLukXSfpIvT8MMkrZT0gKRFknYvqwYzMxtYZbY0ngFOiohjgeOAd0p6IzAH+FJEHA48CUwtsQYzMxtApYVGFDpT74j0CuAk4IY0fAFwalk1mJnZwCr1moakYZLuBjYAtwG/BTZFRFeaZC1wcJk1mJnZwOnTr9z2VURsBY6TNBq4CRhfa7Ja80qaBkwDaGlpoVKplFXmLqOv26izs7Nf29X/FjZQvM82nlJDo1tEbJJUAd4IjJY0PLU2DgHW9TLPPGAeQGtra/T155CbzvJlff7J6P78zHR/1mNWk/fZhlTm3VMvSy0MJO0JvA3oANqBD6TJpgA3l1WDmZkNrDJbGmOBBZKGUYTT4ohYKul+4DpJlwA/B+aXWIOZmQ2g0kIjIu4F/qbG8IeA48tar5mZlUcRNa9DDymtra2xatWqepcxpL1mwWsGbV2/mPKLQVuX7bq8zw4dklZHRGvOtINyIdzK93THbD9v2RqK99nG5N+eMjOzbA4NMzPL5tAwM7NsDg0zM8vm0DAzs2wODTMzy+bQMDOzbA4NMzPL5tAwM7NsDg0zM8vm0DAzs2wODTMzy+bQMDOzbA4NMzPLVubjXl8uqV1Sh6T7JH0qDR8j6TZJD6S/+5VVg5mZDawyn6fRBXw2Iu6StA+wWtJtwNnAioiYLel84HzgvBLraBr9em7A8r7Ns++eI/q+DjPbZZT5uNf1wPrU/bSkDuBg4BSgLU22AKjg0NhpfX2YDRQh05/5zKx5Dco1DUnjKJ4XvhJoSYHSHSwHDkYNZma280p/3KukUcB3gE9HxJ8k5c43DZgG0NLSQqVSKa3GZubtavXU1/2vs7OzX/us9/OBU2poSBpBERjfjogb0+BHJY2NiPWSxgIbas0bEfOAeQCtra3R1+cCW4bly/r8vGWzAdOP/a8/zwj3fj6wSgsNFU2K+UBHRFxWNWoJMAWYnf7eXFYNZja0+eaNxlNmS+PNwIeBX0i6Ow27gCIsFkuaCvweOK3EGsxsiPLNG42pzLun7gB6u4Axsaz1mplZefyNcDMzy+bQMDOzbA4NMzPL5tAwM7NsDg0zM8vm0DAzs2wODTMzy+bQMDOzbA4NMzPL5tAwM7NsDg0zM8vm0DAzs2wODTMzy+bQMDOzbA4NMzPL5tAwM7NspYWGpKskbZD0y6phYyTdJumB9He/stZvZmYDr8yWxtXAO3sMOx9YERGHAytSv5mZNYjSQiMibgc29hh8CrAgdS8ATi1r/WZmNvBKe0Z4L1oiYj1ARKyXdGBvE0qaBkwDaGlpoVKpDE6FTcbb1RqN99n6GuzQyBYR84B5AK2trdHW1lbfgnZFy5fh7WoNxfts3Q323VOPShoLkP5uGOT1m5nZThjs0FgCTEndU4CbB3n9Zma2E8q85XYh8BPgSElrJU0FZgNvl/QA8PbUb2ZmDaK0axoRMbmXURPLWqeZmZXL3wg3M7NsDg0zM8vm0DAzs2wODTMzy+bQMDOzbA4NMzPL5tAwM7NsDg0zM8vm0DAzs2wODTMzy+bQMDOzbA4NMzPL5tAwM7NsDg0zM8vm0DAzs2x1CQ1J75T0a0kPSjq/HjWYmVnfDXpoSBoG/AcwCTgKmCzpqMGuw8zM+q60J/dtx/HAgxHxEICk64BTgPvrUEtTkNT7uDm9zxcRJVRjZo2sHqenDgYeqepfm4ZZSSKi5qu9vb3XcQ4MqydJNV8Pz3l3r+O29+HIBk49Whq1/mVfcoSSNA2YBtDS0kKlUim5rObT2dnp7WpDUnt7e83hnZ2djBo1qtf5vD+Xrx6hsRZ4eVX/IcC6nhNFxDxgHkBra2u0tbUNSnHNpFKp4O1qjcT7bP3V4/TUz4DDJR0maXfgTGBJHeowM7M+GvSWRkR0SfoE8D1gGHBVRNw32HWYmVnf1eP0FBHxXeC79Vi3mZn1n78RbmZm2RwaZmaWzaFhZmbZHBpmZpZNjfDNX0mPAQ/Xu45d0AHA4/UuwqwPvM+W49CIeFnOhA0RGlYOSasiorXedZjl8j5bfz49ZWZm2RwaZmaWzaHR3ObVuwCzPvI+W2e+pmFmZtnc0jAzs2wOjSbl57RbI5F0laQNkn5Z71qanUOjCfk57daArgbeWe8izKHRrLY9pz0ingW6n9NuNiRFxO3AxnrXYQ6NZuXntJtZvzg0mlPWc9rNzHpyaDSnrOe0m5n15NBoTn5Ou5n1i0OjCUVEF9D9nPYOYLGf025DmaSFwE+AIyWtlTS13jU1K38j3MzMsrmlYWZm2RwaZmaWzaFhZmbZHBpmZpbNoWFmZtkcGtb0JF0o6T5J90q6W9IbBmCZ7x2oXw+W1DkQyzEbCL7l1pqapBOAy4C2iHhG0gHA7hGxw2/ISxqevvNSdo2dETGq7PWY5XBLw5rdWODxiHgGICIej4h1ktakAEFSq6RK6r5I0jxJtwLflLRS0tHdC5NUkfR6SWdL+oqkfdOydkvj95L0iKQRkl4pabmk1ZJ+KOnVaZrDJP1E0s8k/esgbw+z7XJoWLO7FXi5pN9IulzSWzPmeT1wSkR8kOJn5U8HkDQWOCgiVndPGBFPAfcA3ct9D/C9iHiO4nnXMyPi9cA/Apenab4MXBERfwv8caffodkAcmhYU4uITooQmAY8BiySdPYOZlsSEX9J3YuB01L36cD1NaZfBJyRus9M6xgFvAm4XtLdwFcpWj0AbwYWpu5r+vSGzEo2vN4FmNVbRGwFKkBF0i+AKUAXL3yoGtljls1V8/5B0hOSXksRDB+rsYolwBcljaEIqB8AewObIuK43srq59sxK5VbGtbUJB0p6fCqQccBDwNrKA7wAO/fwWKuAz4H7BsRv+g5MrVm7qQ47bQ0IrZGxJ+A30k6LdUhScemWX5E0SIBOKvv78qsPA4Na3ajgAWS7pd0L8Uz0y8CLga+LOmHwNYdLOMGioP84u1Mswj4UPrb7SxgqqR7gPt44ZG7nwI+LulnwL59eztm5fItt2Zmls0tDTMzy+bQMDOzbA4NMzPL5tAwM7NsDg0zM8vm0DAzs2wODTMzy+bQMDOzbP8f3HpqkMlTNskAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Box Plot for Age\n",
    "dataset.boxplot(column='Age', by='Survived')\n",
    "plt.title('')\n",
    "plt.ylabel('Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Age variable contains missing data, there is a need to fill in these missing fields. The histogram of the variable is bell-shaped, meaning that using the mean or median to fill in the missing fields may not be a good idea. A better way could be to generate random normal variable values, using the mean and standard deviation estimated from the data. The boxplot of the age for the 2 target classes indicate that the Age variable has approximately the same distribution for both classes, so randomly generating the Age values based on a normal distribution could work in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As mentioned before, it does not make sense to include 'Ticket' (ticket number), so it will not be included in the subsequent processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Overall Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From your own exploration of the data, provide a few paragraphs in summary of the dataset. At this point, it may be helpful to provide a narrative which can reconstruct the situation aboard the titanic as it was sinking. This is also an opportunity to direct your attention towards areas where you feel information is raw and can be improved in your next section, through feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Overall, it appears that some variables are more significant than others, but we would imagine that females and children, especially those who embarked at 'C' and are affluent, hence being able to pay for expensive tickets, will survive, while young men whom travelled alone with no kids or parents are most likely to have perished. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3. Model Interpretation and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, we will begin by learning to appreciate the model interpretation methods related to decision trees and random forests. Please do some of your own research about these approaches. Then, we will move on to do some feature engineering - hopefully this will give us some information gain with respect to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Feature importance\n",
    "Plot a graph/table of feature importance of variables. Is there anything to be expected out of the data? Is there anything unexpected? Compare these findings with your teammate - are there any major differences in these plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Partial Dependence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another useful interpretation plot is partial dependence. `sklearn` might not have a workable library out of the box, so one option would be to try `pdpbox`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, you should first briefly explain your thought process - what is good, what is lacking, and what are the potential areas of information the model has yet to exploit. Following which, do some feature engineering. After every engineered feature, re-run your model and observe if there is an improvement in scores.\n",
    "\n",
    "Running your feature importance again at different points in time can help to validate if your variables are truly important, or are they simply collinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4. Model Re-Training and Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model Re-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When you are confident of your variables, re-run your model with all your variables again, and observe your feature importance. At times having extra variables may even deprove scores. You may also wish to remove features that show insignificant partial dependence. \n",
    "\n",
    "How much accuracy did these engineered features give? How important were these features? At this point in time, you may wish to talk to your peers and identify features they came up with (original ones, not those taken from the internet). This is a stage where brainstorming and contextual knowledge is extremely helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model Tuning - Good for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we should do some model tuning. We previously ran a \"default\" model, with no customization inside our RandomForestClassifier model. However, if we were to look at the parameters, we'll see that there are many you can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': 1,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Evidently, there are many variables worth checking out. For now, some of the most salient ones are `max_depth`, `max_leaf_nodes`, `max_features` and `n_estimators`. These are in general, all parameters we tweak to decrease overfitting. Try tuning these parameters, plotting a graph of model accuracy against parameter variation for each variable.\n",
    "\n",
    "Other useful parameters are `oob_score`, which serves as a validation set of unsampled data points during the bootstrap, and `n_jobs`, which parallelises the process. We recommend you set `oob_score` to `True` (and use the oob_score as a metric), and `n_jobs` to `-1` to speed up your training process.\n",
    "<br /><br />\n",
    "<font color=red>This is not a prerequisite per se, but at this point, you should try to understand the bootstrapping concept. After all, this single concept gave rise to random forests and many other statistical methods we know today!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Subsampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `fastai` library has a very cool method called `set_rf_samples`, which sets the number of subsamples we use in each tree we initalize. For more information, you may refer [here on stackoverflow](https://stackoverflow.com/questions/44955555/how-can-i-set-sub-sample-size-in-random-forest-classifier-in-scikit-learn-espec). You might wish to play with this variable, as it can give you some improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Cross Validation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we think about what we do with validation, we're actually taking a portion (20%) of our data out of our training set for validation purposes. This means that we are sacrificing training data (and hence predictive power) to create a less overfitted, more generalised model. There is a trade-off for our model: we remove overfitting (variance) by sacrificing predictive power (increasing bias). This is known as the bias variance trade-off, which we will go into more detail next week.\n",
    "\n",
    "We will go into details next week, but in short, this can be avoided using cross validation. If you have done this before, you may use cross validation to improve the model here. Report your accuracy.\n",
    "\n",
    "Otherwise, if we know all the validation scores for all our models, simply pick the best model in terms of validation score. Report your accuracy. \n",
    "\n",
    "Put back all our data into one big training set, and re-train the model using this training set. You can now make a prediction on your test set, and submit your result to Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is your Kaggle leaderboard performance? Please provide your Kaggle username as well. We hope you have had a prediction accuracy of at least 78%, but it's okay if you don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions[['PassengerId', 'Survived']].to_csv('data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Accuracy:\n",
    "# Kaggle name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Not all models are perfect, especially not in the constraint of time. Do some research on the models that do better than you, and list out the areas that you can improve on in the long run. Prioritise these improvements and spell out how you can implement them if they are non-trivial to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Ensembling/stacking instead of using a single RF model\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 5. Building your Random Forest from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Congratulations! You have completed the tutorial on random forests... not!\n",
    "\n",
    "Apart from modelling, each week, you will also be expected to implement the models we are using. After all, the best way to learn is to implement from scratch. AI Apprentices are not only expected to model, but also do the necessary engineering for real life problems, and many such problems require custom code. For example, we may want to use subsampling to improve our model performance, but edge-cutting methods would not yet be available in common libraries. When this happens, you will have to address these problems yourself.\n",
    "\n",
    "Numerical programming might be new to some, if you did not come from the R/Matlab side of things. To get yourself up to speed with numerical programming in Python, we highly recommend Wes McKinney's [Python for Data Analysis, 2nd Ed.](https://www.safaribooksonline.com/library/view/python-for-data/9781491957653/)\n",
    "\n",
    "__Note__: In this guided implementation, we made 2 decisions, firstly to use a Python `class`, i.e. object oriented programming (arguably so at least), and secondly to use the `numpy` library. Neither of these decisions are compulsory - if you have prior experience in another style, or using alternative libraries, feel free to do so, and modify the script to allow your code to run. However, if you have no prior experience, we suggest sticking to this format - we will follow `scikit-learn`'s format, which we believe is increasingly an industry standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A random forest, as the the name suggests, is made up of many decision trees, each with levels of variation and randomness. Before looking at random forests, we will look at understanding what decision trees do.\n",
    "\n",
    "Decision trees, more specifically Classification and Regression Trees (CARTs), are an algorithm/data structure that learns to split data out based on rules it learns. There are many resources out there to get a good understanding of what CARTs are, which you may wish to reference while accomplishing the tasks here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Gini Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you remember from `get_params`, there exists a parameter `criterion: 'gini'`. This means that the tree is using Gini as a criterion to decide how to separate the data.\n",
    "\n",
    "Hence, we will first learn how to use the Gini impurity score. The Gini impurity score of a node n is given as:  \n",
    "\n",
    "<center>$i(n) = 1 - p^2_0 - p^2_1$,  </center>  \n",
    "\n",
    "Where $p_1$ refers to the proportion of 1's in that node, and $p_0$ refers to the proportion of 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from src.decision_tree import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the above line of code to work, you will have to do the following if you haven't done so:\n",
    "1. Create a folder called src at the directory of your current notebook\n",
    "2. Create a __init__.py empty file in the src folder -see http://mikegrouchy.com/blog/2012/05/be-pythonic-__init__py.html\n",
    "3. Create a file, `decision_tree.py`. You can consider the terminal script `touch decision_tree.py`\n",
    "4. create a class `DecisionTree` inside `deicison_tree.py`\n",
    "\n",
    "You may realise that for this part of the coursework, we are not writing code directly into Jupyter notebooks, but inside the /src/ folder as `.py` files. We are maintaining a code base, outside of the Jupyter notebook. We do this for two reasons - 1) because this code is highly reusable in future sessions, beyond the scope of one notebook. 2) because such code bases are collaboration-friendly, as Git and Jupyter notebooks do not play well with each other, but python files do. In the future, non-exploratory code will be written in teams, so scripts would be a more collaboration friendly format. The `src/` folder structure is a very basic and light introduction to this, but in short, each project should have a different folder structure to cater to its needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def approx_eq(a, b, num_sig=5):\n",
    "    return round(a, num_sig) == round(b, num_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "approx_eq(DecisionTree().gini([1, 0, 0, 0, 0], [1, 1, 1, 1, 0]), .32)\n",
    "# for the above line of code to work,\n",
    "# 1. create a method gini that takes in 2 arrays and computes the node's gini impurity\n",
    "# 2. implement the method as per the mathematical formula given\n",
    "# 3. if you would like to turn this into a private method, make the necessary adjustments\n",
    "# -> DecisionTree()._DecisionTree__gini()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Following sklearn's `fit` and `predict`/`score` approach to programming, we will be implementing the fit and predict methods. First, we will attempt to implement a fit method.\n",
    "\n",
    "The fit method will take in 2 numpy matrices: a m\\*n train array with m training examples of n features, and a m\\*1 array of labels.\n",
    "\n",
    "There are tons of resources available to describe the workings of a CART. We would encourage you to find a source that best suits your needs, but we have picked out two points which other resources may miss at the implementation stage. Feel free to find more resources to expand on these areas:\n",
    "\n",
    "1. The CART is a recursive tree structure. Every node of the tree can be seen as a decision tree node. When it splits, its left and right branches and its child nodes. When fitting a tree, you should recursively fit the nodes of the tree, in a way that the fitting can be used to predict in the future.\n",
    "\n",
    "2. In finding the best condition to split the variables, it is alright to iterate through every single unique value of every variable, and determine the best condition through the iterations. The best condition can be defined as the one that provides the most __information gain__, which is defined as the greatest loss in Gini impurity.\n",
    "\n",
    "If this is your first time doing object oriented programming in Python, we would high recommend you expose yourself to some Python resources first, or read the Python documentation. __If you need more help with these methods, ask a peer with programming experience, or you can seek help from the team.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# read a new csv and remove complicated columns\n",
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "X_cols = titanic.columns\n",
    "X_cols = X_cols.drop('Age')\n",
    "X_cols = X_cols.drop('Cabin')\n",
    "X_cols = X_cols.drop('Name')\n",
    "X_cols = X_cols.drop('Ticket')\n",
    "titanic = titanic[X_cols]\n",
    "\n",
    "# one hot encoding for remainining multiclass columns\n",
    "titanic['Sex_m'] = (titanic['Sex'] == 'male').astype('int')\n",
    "titanic['Embarked_S'] = (titanic['Embarked'] == 'S').astype('int')\n",
    "titanic['Embarked_C'] = (titanic['Embarked'] == 'C').astype('int')\n",
    "titanic = titanic.drop(['Sex', 'Embarked'], axis=1)\n",
    "\n",
    "# create X and y, test and train\n",
    "X_cols = titanic.columns\n",
    "X_cols = X_cols.drop('Survived')\n",
    "X_titanic = titanic[X_cols]\n",
    "y_titanic = titanic['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic, y_titanic, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you have designed your `fit` method well, predict method will be naturally easy. If the node is a leaf, simply return the leaf value. If the node is not a leaf, call predict on one of its child nodes depending on whether it fits the condition.\n",
    "\n",
    "__If you need more help with these methods, ask a peer with programming experience, or you can seek help from the team.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_dt = dt.predict(X_test.values)\n",
    "sum(preds_dt == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have a decision tree, we can build a random forest, comprising of decision trees of randomised bootstraps of our dataset. At the simplest level, a random forest can be simply a list of decision trees that take a vote on the outcome of the prediction. This list can be an attribute of the random forest.\n",
    "\n",
    "The basic modification of random forests is the use of bootstrapping. Bootstrapping is done in a few lines of code through `np.random.choice`.\n",
    "\n",
    "Hence, to begin, build a simple random forest, that will initialise 5 trees through bootstrapping (sampling 100% with replacement), and predict the answer through a voting mechanism out of all the 5 trees. For computational efficiency, we recommend using `np.stack` and `np.array.mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_0 = RandomForest()\n",
    "rf_0.fit(X_train.values, y_train.values)\n",
    "preds_rf = rf_0.predict(X_test.values)\n",
    "sum(preds_rf == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we will implement `n_trees` to be tweakable. In addition, we will have a `subsample_size` parameter, which does the subsampling that the sklearn's random forest could not do. We can continue to use `np.random.choice`, but if subsample_size > 1, we can sample without replacement instead. (Or you could have another parameter to adjust that too.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_1 = RandomForest(n_trees=10, subsample_size=0.8)\n",
    "rf_1.fit(X_train.values, y_train.values)\n",
    "preds_rf1 = rf_1.predict(X_test.values)\n",
    "sum(preds_rf1 == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we will implement the `feature_proportion` feature, which refers to the number of features we allow each tree to use. This further increases the randomness and eliminates overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_2 = RandomForest(n_trees=100, subsample_size=0.5, feature_proportion=0.5)\n",
    "rf_2.fit(X_train.values, y_train.values)\n",
    "preds_rf2 = rf_2.predict(X_test.values)\n",
    "sum(preds_rf2 == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You may wish to attempt to implement other optional parameters of random forest. One important parameter is `max_features` which makes the tree lose some features at every node, or `max_depth`, which limits the number of levels the tree can have. However, we chose to leave these out, as they require tweaking at the decision tree level, which is an exercise left for your own choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Congratulations!__ You have finally come to the end of the week 1. Hope you had as much fun as we had building it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://www.ambitiouskitchen.com/wp-content/uploads/2014/03/glutenfreecookies-6.jpg\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
